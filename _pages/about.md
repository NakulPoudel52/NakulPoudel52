---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

My name is Nakul Poudel, and I am a third-year PhD student at the [Chester F. Carlson Center for Imaging Science](https://www.rit.edu/science/chester-f-carlson-center-imaging-science) at the Rochester Institute of Technology (RIT), Rochester, New York, USA. I conduct my research under the supervision of [Dr. Cristian A. Linte](https://www.rit.edu/directory/calbme-cristian-linte) in the Biomedical Modeling, Visualization, and Image-Guided Navigation (BiMVisGN) Lab. My research focuses on deep learning methods for medical imaging applications.

Prior to starting my PhD, I obtained my Bachelor’s degree in Computer Engineering from [Sagarmatha Engineering College](https://sagarmatha.edu.np/), affiliated with Tribhuvan University, Nepal. Outside of my research, I enjoy traveling, exploring new places, hiking, and experiencing different cultures.

Research
======
My research focuses on developing advanced deep learning methodologies for medical imaging and surgical data analysis. I am particularly interested in designing robust, generalizable, and clinically meaningful computational models that bridge the gap between artificial intelligence and real-world surgical applications. 

During my PhD, I have worked on several foundational problems in medical artificial intelligence, including image and surface registration for aligning preoperative and intraoperative data, organ surface reconstruction and completion from sparse, partial, and noisy observations, and segmentation and detection of anatomical structures and surgical instruments. A significant aspect of my research involves addressing real-world challenges such as non-canonical poses, partial visibility, noise, and domain shifts between imaging modalities.

Beyond solving individual tasks, my broader research vision is to move toward general-purpose surgical AI systems capable of understanding and reasoning about the entire surgical workflow. Modern operating rooms generate rich visual and geometric data streams, yet most existing approaches remain narrowly specialized for single tasks. I am interested in developing unified surgical vision models that can simultaneously interpret surgical scenes, detect and classify tools, recognize tool–tissue interactions, identify surgical phases, recognize procedural steps, assess surgical skill, and detect potential errors. Such models would enable a more holistic understanding of surgical procedures rather than treating each task in isolation.

Currently, I am exploring surgical vision foundation models designed to learn shared representations across multiple tasks and domains. By integrating large-scale pretraining, multi-task learning, and vision–language modeling strategies, I aim to design a single unified architecture capable of generalizing across diverse surgical scenarios. My long-term goal is to contribute to the development of intelligent, context-aware surgical assistance systems that enhance intraoperative decision-making, improve safety, and ultimately advance patient outcomes.

News
======
1. [Dec 2025] Our paper "Assessing Learning-Based Reconstructed Liver Surfaces from Partial Point Clouds for Improving Pre- to Intra-Operative 3D–3D Registration”
has been published in Wiley's IET Healthcare Technology Letters journal.
1. [Aug 2025] Accepted to present my research titled “Assessing Learning-Based Reconstructed Liver Surfaces from Partial Point Clouds for Improving Pre- to Intra-Operative 3D–3D Registration” at the [AE-CAI Workshop](https://workshops.ap-lab.ca/aecai2025/) at the [Medical Image Computing and Computer-Assisted Intervention (MICCAI)](https://conferences.miccai.org/2025/en/) 2025 in Daejeon, South Korea.
1. [July 2025] Delivered an oral presentation of our paper, “Toward Patient-Specific Partial Point Cloud to Surface Completion for Pre- to Intra-Operative Registration in Image-Guided Liver Interventions”, at the [Medical Image Understanding and Analysis (MIUA)](https://conferences.leeds.ac.uk/miua/) 2025 held at the [University of Leeds](https://www.leeds.ac.uk/), Leeds, UK.
1. [May 2025] Excited to share that our paper, “Toward Patient-Specific Partial Point Cloud to Surface Completion for Pre- to Intra-Operative Registration in Image-Guided Liver Interventions”, has been accepted at the [Medical Image Understanding and Analysis (MIUA)](https://conferences.leeds.ac.uk/miua/) Annual Conference!
1. [Feb 2025] Presented our paper as a poster presentation at SPIE Medical Imaging 2025, held in San Diego, California, USA.
1. [Oct 2024] Our paper titled “Evaluation of Intraoperative Patient-Specific Methods for Point Cloud Completion for Minimally Invasive Liver Interventions” has been accepted to SPIE Medical Imaging 2025.
1. [May 2024] Appointed as Treasurer of the [Nepalese Student Association](https://campusgroups.rit.edu/nsarit/home/) at the Rochester Institute of Technology.
1. [Aug 2023] Started my PhD in Imaging Science at the Rochester Institute of Technology, Rochester, United States.
1. [July 2022] Graduated with a Bachelor’s degree in Computer Engineering from Sagarmatha Engineering College, affiliated with Tribhuvan University, Nepal.
1. [April 2022] [Selected](https://www.facebook.com/share/p/17GnQRyPzD/) for the Erasmus+ program to study a semester at UPCT Universidad Politécnica de Cartagena.   
1. [June 2020] Our team “Matrix” secured [First Runner-Up](https://www.facebook.com/story.php?story_fbid=3022269811172201&id=100063642486901) position in the Hack for Good Online Hackathon organized by Sagarmatha Engineering College, Kathmandu, Nepal.

<!-- Create content & metadata
------
For site content, there is one Markdown file for each type of content, which are stored in directories like _publications, _talks, _posts, _teaching, or _pages. For example, each talk is a Markdown file in the [_talks directory](https://github.com/academicpages/academicpages.github.io/tree/master/_talks). At the top of each Markdown file is structured data in YAML about the talk, which the theme will parse to do lots of cool stuff. The same structured data about a talk is used to generate the list of talks on the [Talks page](https://academicpages.github.io/talks), each [individual page](https://academicpages.github.io/talks/2012-03-01-talk-1) for specific talks, the talks section for the [CV page](https://academicpages.github.io/cv), and the [map of places you've given a talk](https://academicpages.github.io/talkmap.html) (if you run this [python file](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.py) or [Jupyter notebook](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.ipynb), which creates the HTML for the map based on the contents of the _talks directory).
 -->
